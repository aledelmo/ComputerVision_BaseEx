{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "!unzip \"/content/drive/MyDrive/Parrot/img.zip\" -d \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from IPython.utils import io\n",
    "\n",
    "from packaging import version\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "gpu_list = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPUs Available:\")\n",
    "if gpu_list:\n",
    "    try:\n",
    "        for gpu in gpu_list:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpu_list), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print('None')\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "from tensorflow.keras import mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "print('Compute dtype: %s' % policy.compute_dtype)\n",
    "print('Variable dtype: %s' % policy.variable_dtype)\n",
    "\n",
    "debug_mode = False\n",
    "tf.config.run_functions_eagerly(debug_mode)\n",
    "if debug_mode:\n",
    "    tf.data.experimental.enable_debug_mode()\n",
    "\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "data_path = './img/'\n",
    "train_ds = tf.data.Dataset.list_files(os.path.join(data_path, 'train', '*.png'))\n",
    "test_ds = tf.data.Dataset.list_files(os.path.join(data_path, 'test', '*.png'))\n",
    "\n",
    "def process_path(file_path):\n",
    "    label = tf.strings.to_number(tf.strings.split(file_path, '_')[1])\n",
    "    image = tf.io.decode_png(tf.io.read_file(file_path))\n",
    "    return image, label\n",
    "\n",
    "train_ds = train_ds.map(process_path,\n",
    "                         num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "                         deterministic=False\n",
    "                        ).batch(batch_size,\n",
    "                                num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "                                deterministic=False).prefetch(tf.data.experimental.AUTOTUNE).cache()\n",
    "\n",
    "test_ds = test_ds.map(process_path,\n",
    "                         num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "                         deterministic=True\n",
    "                        ).batch(batch_size,\n",
    "                                num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "                                deterministic=True).prefetch(tf.data.experimental.AUTOTUNE).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 9\n",
    "img_width = 9\n",
    "channels = 3\n",
    "num_classes = 2\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Rescaling(1./255, input_shape=(img_height, img_width, channels)),\n",
    "  tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  tf.keras.layers.BatchNormalization(axis=-1),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  tf.keras.layers.BatchNormalization(axis=-1),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  tf.keras.layers.BatchNormalization(axis=-1),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(num_classes)\n",
    "])\n",
    "\n",
    "optimizer = mixed_precision.LossScaleOptimizer(tf.keras.optimizers.Adam(learning_rate=1e-5))\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric_fn = tf.keras.metrics.Accuracy()\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "log_dir = './tf_logs/'\n",
    "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\n",
    "test_metric = tf.keras.metrics.Mean('test_metric', dtype=tf.float32)\n",
    "summary_writer_train = tf.summary.create_file_writer(os.path.join(log_dir, 'Train'))\n",
    "summary_writer_test = tf.summary.create_file_writer(os.path.join(log_dir, 'Test'))\n",
    "progression_bar = tf.keras.utils.Progbar(target=epochs, verbose=1, unit_name='epoch',\n",
    "                                                 stateful_metrics=['Train Loss'])\n",
    "\n",
    "@tf.function()\n",
    "def _train_step(_x_train, _y_train):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(_x_train, training=True)\n",
    "        loss = loss_fn(_y_train, predictions)\n",
    "        scaled_loss = optimizer.get_scaled_loss(loss)\n",
    "    scaled_gradients = tape.gradient(scaled_loss, model.trainable_variables)\n",
    "    gradients = optimizer.get_unscaled_gradients(scaled_gradients)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "\n",
    "@tf.function()\n",
    "def _test_step(_x_test, _y_test):\n",
    "    predictions = model(_x_test, training=False)\n",
    "    loss = loss_fn(_y_test, predictions)\n",
    "    test_loss(loss)\n",
    "    #metric = metric_fn(_y_test, predictions)\n",
    "    #test_metric(metric)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tf.summary.experimental.set_step(epoch)\n",
    "    for (x_train, y_train) in train_ds:\n",
    "      _train_step(x_train, y_train)\n",
    "    for (x_test, y_test) in test_ds:\n",
    "         _test_step(x_test, y_test)\n",
    "    with summary_writer_train.as_default():\n",
    "        tf.summary.scalar('Loss', train_loss.result(), step=epoch)\n",
    "    with summary_writer_test.as_default():\n",
    "        tf.summary.scalar('Loss', test_loss.result(), step=epoch)\n",
    "    progression_bar.update(current=epoch + 1,\n",
    "                           values=[('Train Loss', train_loss.result()),\n",
    "                                   ('Test Loss', test_loss.result())])\n",
    "    train_loss.reset_states()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Parrot",
   "language": "python",
   "name": "parrot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}