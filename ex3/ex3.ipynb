{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive', force_remount=True)\n",
    "#!unzip \"/content/drive/MyDrive/Parrot/img.zip\" -d \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from IPython.utils import io\n",
    "\n",
    "from packaging import version\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "gpu_list = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPUs Available:\")\n",
    "if gpu_list:\n",
    "    try:\n",
    "        for gpu in gpu_list:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpu_list), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print('None')\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "from tensorflow.keras import mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "print('Compute dtype: %s' % policy.compute_dtype)\n",
    "print('Variable dtype: %s' % policy.variable_dtype)\n",
    "\n",
    "debug_mode = False\n",
    "tf.config.run_functions_eagerly(debug_mode)\n",
    "if debug_mode:\n",
    "    tf.data.experimental.enable_debug_mode()\n",
    "\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
    "\n",
    "!rm -rf ./tf_logs/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_path = '../data/img/'\n",
    "train_ds = tf.data.Dataset.list_files(os.path.join(data_path, 'train', '*.png'))\n",
    "test_ds = tf.data.Dataset.list_files(os.path.join(data_path, 'test', '*.png'))\n",
    "\n",
    "def process_path(file_path):\n",
    "    label = tf.strings.to_number(tf.strings.split(file_path, '_')[1])\n",
    "    image = tf.io.decode_png(tf.io.read_file(file_path))\n",
    "    return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./tf_logs/hparam_tuning --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 9\n",
    "img_width = 9\n",
    "channels = 3\n",
    "num_classes = 2\n",
    "\n",
    "def get_model(base_filters, dense_filters, n_convs = 3, n_dense = 2):\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Rescaling(1./255, input_shape=(img_height, img_width, channels)))\n",
    "    m = 1\n",
    "    for i in range(n_convs):\n",
    "        model.add(tf.keras.layers.Conv2D(base_filters * m, 3, padding='same', activation='relu'))\n",
    "        model.add(tf.keras.layers.BatchNormalization(axis=-1))\n",
    "        model.add(tf.keras.layers.MaxPooling2D())\n",
    "        m += m\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    if n_dense > 1:\n",
    "        m = 2 * (n_dense - 1)\n",
    "    else:\n",
    "        m = 1\n",
    "    for i in range(n_dense):\n",
    "        model.add(tf.keras.layers.Dense(dense_filters * m, activation='relu'))\n",
    "        m = m // 2\n",
    "    model.add(tf.keras.layers.Dense(num_classes))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_optimizer(initial_lr):\n",
    "    return mixed_precision.LossScaleOptimizer(tf.keras.optimizers.Adam(learning_rate=initial_lr))\n",
    "\n",
    "def train(model, optimizer, epochs, batch_size, logdir,train_ds, test_ds):\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metric_fn = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "    train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "    test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\n",
    "    test_metric = tf.keras.metrics.Mean('test_metric', dtype=tf.float32)\n",
    "\n",
    "    @tf.function()\n",
    "    def _train_step(model, optimizer, _x_train, _y_train):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(_x_train, training=True)\n",
    "            loss = loss_fn(_y_train, predictions)\n",
    "            scaled_loss = optimizer.get_scaled_loss(loss)\n",
    "        scaled_gradients = tape.gradient(scaled_loss, model.trainable_variables)\n",
    "        gradients = optimizer.get_unscaled_gradients(scaled_gradients)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        train_loss(loss)\n",
    "\n",
    "    @tf.function()\n",
    "    def _test_step(model, _x_test, _y_test):\n",
    "        predictions = model(_x_test, training=False)\n",
    "        loss = loss_fn(_y_test, predictions)\n",
    "        test_loss(loss)\n",
    "        metric = metric_fn(tf.one_hot(tf.cast(_y_test, tf.int32), 2), predictions)\n",
    "        test_metric(metric)\n",
    "\n",
    "    log_dir = logdir\n",
    "\n",
    "    summary_writer_train = tf.summary.create_file_writer(os.path.join(log_dir, 'Train'))\n",
    "    summary_writer_test = tf.summary.create_file_writer(os.path.join(log_dir, 'Test'))\n",
    "    progression_bar = tf.keras.utils.Progbar(target=epochs, verbose=1, unit_name='epoch',\n",
    "                                                     stateful_metrics=['Train Loss'])\n",
    "\n",
    "    train_ds = train_ds.map(process_path,\n",
    "                         num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "                         deterministic=False\n",
    "                        ).batch(batch_size,\n",
    "                                num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "                                deterministic=False).prefetch(tf.data.experimental.AUTOTUNE).cache()\n",
    "\n",
    "    test_ds = test_ds.map(process_path,\n",
    "                         num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "                         deterministic=True\n",
    "                        ).batch(batch_size,\n",
    "                                num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "                                deterministic=True).prefetch(tf.data.experimental.AUTOTUNE).cache()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        tf.summary.experimental.set_step(epoch)\n",
    "        for (x_train, y_train) in train_ds:\n",
    "            _train_step(model, optimizer, x_train, y_train)\n",
    "        for (x_test, y_test) in test_ds:\n",
    "            _test_step(model, x_test, y_test)\n",
    "        with summary_writer_train.as_default():\n",
    "            tf.summary.scalar('Loss', train_loss.result(), step=epoch)\n",
    "        with summary_writer_test.as_default():\n",
    "            tf.summary.scalar('Loss', test_loss.result(), step=epoch)\n",
    "            tf.summary.scalar('Accuracy', test_metric.result(), step=epoch)\n",
    "        progression_bar.update(current=epoch + 1,\n",
    "                               values=[('Train Loss', train_loss.result()),\n",
    "                                       ('Test Loss', test_loss.result()),\n",
    "                                       ('Test Metric', test_metric.result())])\n",
    "        train_loss.reset_states()\n",
    "        test_loss.reset_states()\n",
    "        test_metric.reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "session_num = 0\n",
    "run_name = 'run'\n",
    "\n",
    "HP_EPOCHS = hp.HParam('epochs', hp.Discrete([150]))\n",
    "HP_BATCH = hp.HParam('batch', hp.Discrete([8]))\n",
    "HP_LR = hp.HParam('initial_lr', hp.Discrete([1e-5]))\n",
    "HP_FILTERS = hp.HParam('base_filters', hp.Discrete([128]))\n",
    "HP_DENSEFILTERS = hp.HParam('dense_filters', hp.Discrete([8]))\n",
    "HP_N_CONV = hp.HParam('n_conv_filters', hp.Discrete([1]))\n",
    "HP_N_DENSE = hp.HParam('n_dense_filters', hp.Discrete([1]))\n",
    "\n",
    "def hyper_parameters_config():\n",
    "    for e in HP_EPOCHS.domain.values:\n",
    "        for lr in HP_BATCH.domain.values:\n",
    "            for a in HP_LR.domain.values:\n",
    "                for bf in HP_FILTERS.domain.values:\n",
    "                    for d in HP_DENSEFILTERS.domain.values:\n",
    "                        for nc in HP_N_CONV.domain.values:\n",
    "                            for nd in HP_N_DENSE.domain.values:\n",
    "                                yield {\n",
    "                                    HP_EPOCHS: e,\n",
    "                                    HP_BATCH: lr,\n",
    "                                    HP_LR: a,\n",
    "                                    HP_FILTERS: bf,\n",
    "                                    HP_DENSEFILTERS: d,\n",
    "                                    HP_N_CONV: nc,\n",
    "                                    HP_N_DENSE: nd,\n",
    "                                }\n",
    "\n",
    "for hparams in hyper_parameters_config():\n",
    "    run_name = 'run-{}'.format(session_num)\n",
    "    print('\\n\\n--- Starting trial: {}'.format(run_name))\n",
    "    print({h.name: hparams[h] for h in hparams})\n",
    "\n",
    "    run_logdir = 'tf_logs/hparam_tuning/' + run_name\n",
    "    summary_writer = tf.summary.create_file_writer(run_logdir)\n",
    "\n",
    "    with summary_writer.as_default():\n",
    "        hp.hparams(hparams)\n",
    "        model = get_model(hparams[HP_FILTERS], hparams[HP_DENSEFILTERS], hparams[HP_N_CONV], hparams[HP_N_DENSE])\n",
    "        optimizer = get_optimizer(hparams[HP_LR])\n",
    "        train(model, optimizer, hparams[HP_EPOCHS], hparams[HP_BATCH],\n",
    "              run_logdir, train_ds = train_ds, test_ds=test_ds)\n",
    "    session_num += 1\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}